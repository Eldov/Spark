{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Streaming:\n",
    "\n",
    "**Micro Batch Processing** -> means each selected interval (let's say 2 seconds) the data is collected and sent to be processed.  \n",
    "The processing time should be fast enough to be ready to proccess the upcoming batch.  \n",
    "The new input will be added as a row in an input table and then once finished, delivered to a results table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying transformations to streaming DataFrame:  \n",
    "spark.readStream\n",
    "    #<insert input configuration>\n",
    "    .filter(col(\"col_1\")==\"finalize\")\n",
    "    .groupBy(\"col_2\").count()\n",
    "\n",
    "#Configuring a data stream writer:\n",
    "spark.readStream\n",
    "    #<insert input configuration>\n",
    "    .filter(col(\"col_1\")==\"finalize\")\n",
    "    .groupBy(\"col_2\").count()\n",
    "    .writeStream\n",
    "    #<insert sink configurations>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Modes:\n",
    "- **Append** -> Add new records only\n",
    "- **Update** -> Update changed records in place\n",
    "- **Complete** -> Rewrite full output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigger Types:\n",
    "- **Default** -> Process each micro-batch as soon as the previous one has been processed\n",
    "- **Fixed interval** -> Micro-batch processing kicked off at the user-specified interval\n",
    "- **One-time** -> Process all of the available data as a single micro-batch and then automatically stop the query\n",
    "- **Continuous Processing** -> Long-running tasks that continuously read, process, and write data as soon events are available *experimental, Spark 2.3+*\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Fault Tolerance:\n",
    "Guaranteed in Structured Streaming by:  \n",
    "- Checkpointing and write-ahead logs  \n",
    "- Idempotent sinks  \n",
    "- Replayable data sources"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Query Operations\n",
    "- **Stop stream**\n",
    "- **Await termination**\n",
    "- **Status**\n",
    "- **Is active**\n",
    "- **Recent progress**\n",
    "- **Name, ID, runID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To start/create and execute a streaming query:\n",
    "spark.readStream\n",
    "    #<insert input configuration>\n",
    "    .filter(col(\"col_1\")==\"finalize\")\n",
    "    .groupBy(\"col_2\").count()\n",
    "    .writeStream\n",
    "    #<insert sink configurations>\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete streaming query\n",
    "spark.readStream\n",
    "    .schema(dataSchema)\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .parquet (eventsPath)\n",
    "    .filter(col(\"event_name\") == \"finalize\")\n",
    "    .groupBy(\"traffic_source\").count()\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"parquet\")\n",
    "    .queryName(\"program_ratings\")\n",
    "    .trigger(processing Time=\"3 seconds\")\n",
    "    .option(\"checkpointLocation\", checkpointPath)\n",
    "    .start(outputPathDir)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
