{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE A TABLE IF NOT EXISTS table_name USING parquet OPTIONS (path \"/the/file/path/table_file.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "spark.sql(\"\"\"CREATE A TABLE IF NOT EXISTS table_name USING parquet OPTIONS (path \"{}\")\"\"\".format(table_name_path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a widget:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE WIDGET TEXT widget_name DEFAULT \"widget value\"\n",
    "\n",
    "getArgument(\"widget_name\") #Returns the widget\n",
    "\n",
    "REMOVE WIDGET widget_name # Eliminate the widget"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSession (Used to be Spark Context in older versions):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SparkSession Methods](pics/SparkSession%20Methods.PNG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL vs DataFrame API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT col_1, col_2\n",
    "FROM table_name\n",
    "WHERE col_2 < 20\n",
    "ORDER BY col_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "display(spark.table(\"table_name\")\n",
    "    .select(\"col_1\", \"col_2\")\n",
    "    .where(\"col_2 < 20\")\n",
    "    .orderBy(\"col_2\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableDF = spark.table(\"table_name\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSession Methods:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **spark.sql()** -> creates a df from a query  \n",
    "- **spark.table()** -> creates a df from a table  \n",
    "- **spark.read()** -> reads data in into dfs  \n",
    "- **spark.range()** -> creates df with a col made of elements in a range  \n",
    "- **spark.createDataFrame()** -> creates a df from tuples  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Action Methods:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **df.show()** -> display top n rows in table form  \n",
    "- **df.count()** -> returns number of rows in the df  \n",
    "- **df.describe()/summary()** -> computes basic statistics \n",
    "- **df.first()/head()** -> returns first row \n",
    "- **df.collect()** -> returns all the df rows in an array \n",
    "- **df.take()** -> returns first n rows in an array \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Operators and Methods:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Column Operators and Methods](pics/Column%20Operators%20and%20Methods.PNG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row Methods:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Row Methods](pics/Row%20Methods.PNG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a View:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"view_name\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Schema Structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_struct_schema = StructType([\n",
    "    StructField(\"col_1\", StringType(), True),\n",
    "    StructField(\"col_2\", LongType(), True),\n",
    "    StructField(\"col_n\", StringType(), True)\n",
    "])\n",
    "\n",
    "#You also have ArrayType, IntegerType, DoubleType, etc\n",
    "#You can also add StructTypes inside of StructFields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ddl\n",
    "df_ddl_schema = \"col_1 string, col_2 long, col_n string\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parquet\n",
    "READ -> df = (spark\n",
    "                .read\n",
    "                .parquet(\"path/to/the.parquet\")\n",
    "            )  \n",
    "WRITE -> (df.write  \n",
    "                .option(\"compression\", \"snappy\")  \n",
    "                .mode(\"overwrite\")  \n",
    "                .parquet(\"path/to/target.parquet\")  \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv\n",
    "READ -> df = (spark\n",
    "                .read\n",
    "                .option(\"sep\", \"\\t\")\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", True)\n",
    "                #or\n",
    "                .schema(df_defined_schema)\n",
    "                .csv(\"path/to/the.csv\")\n",
    "            )  \n",
    "#OR\n",
    "READ -> df = (spark\n",
    "                .read\n",
    "                .csv(\"path/to/the.csv\", sep =\"\\t\", header=True, inferSchema=True)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json\n",
    "READ -> df = (spark\n",
    "                .read\n",
    "                .option(\"inferSchema\", True)\n",
    "                .json(\"path/to/the.json\")\n",
    "            )  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can also write as table\n",
    "WRITE -> df.write.mode(\"overwrite\").saveAsTable(\"table_name\")\n",
    "\n",
    "#or Delta Table\n",
    "WRITE -> df.write.format(\"delta\").mode(\"overwrite\").save(\"path/to/the/delta\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45328cea26b25948b97b0f37f1a12ccfd1fd6868ffd79945aa9cfb5de8c131d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
