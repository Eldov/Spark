{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Transformation Methods:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Column Operators and Methods](pics/DataFrame%20Transformation%20Methods.PNG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation Method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouped data object  \n",
    "Methods for aggregations on a DataFrame, created by df.groupBy()\n",
    "- **agg()** -> Compute aggregates by specifying a series of aggregate columns\n",
    "- **avg()** -> Compute the mean value for each numeric columns for each group\n",
    "- **count()** -> Count the number of rows for each group\n",
    "- **max()** -> Compute the max value for each numeric columns for each group\n",
    "- **mean()** -> Compute the average value for each numeric columns for each group\n",
    "- **min()** -> Compute the min value for each numeric column for each group\n",
    "- **pivot()** -> Pivots a column of the current DataFrame and performs the specified aggregation\n",
    "- **sum()** -> Compute the sum for each numeric columns for each group\n",
    "- **collect_list()** -> Returns an array consisting of all values within the group\n",
    "- **collect_set()** -> Returns an array consisting of all unique values within the group\n",
    "  \n",
    "Example 1: df.groupBy(\"col_1\").count()  \n",
    "Example 2: df.groupBy(\"col_1\").agg(sum(\"col_2\").alias(\"sum_col_2\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math Functions\n",
    "- **ceil()** -> Computes the ceiling of the given column\n",
    "- **log()** -> Computes the natural logarithm of the given value\n",
    "- **round()** -> Returns the value of the column e rounded to O decimal places with HALF_UP round mode\n",
    "- **sqrt()** -> Computes the square root of the specified float value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Time Functions  \n",
    "\n",
    "- **date_format()** -> Converts a date/timestamp/string to a value of string in the format specified by the date format given by the second argument\n",
    "- **add_months** -> Returns the date that is numMonths after startDate\n",
    "- **dayofweek()** -> Extracts the day of the week as an integer from a given date/timestamp/sring\n",
    "- **from_unixtime()** -> Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the yyyy-MM-dd HH:mm:ss format\n",
    "- **minute()** -> Extracts the minutes as an integer from a given date/timestamp/string\n",
    "- **unix_timestamp()** -> Converts time string with given pattern to Unix timestamp (in seconds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Functions\n",
    "- **translate()** -> Translate any character in the src by a character in replaceString\n",
    "- **regexp_replace()** -> Replace all substrings of the specified string value that match regexp with rep\n",
    "- **regexp_extract()** -> Extract a specific group matched by a Java regex, from the specified string column\n",
    "- **Itrim()** -> Removes the leading space characters from the specified string column\n",
    "- **lower()** -> Converts a string column to lowercase\n",
    "- **split()** -> Splits str around matches of the given pattern"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection Functions\n",
    "- **array_contains()** -> Returns null if the array is null, true if the array contains a value, and false otherwise\n",
    "- **element_at()** -> Returns element of array at given index. Array elements are numbered starting with 1\n",
    "- **explode()** -> Creates a new row for each element in the given array or map column\n",
    "- **slice()** -> Returns an array containing all the elements in x from index start (or starting from the end if start is negative) with the specified length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Aggregate Functions\n",
    "- **col/column()** -> Returns a Column based on the given column name\n",
    "- **lit()** -> Creates a Column of literal value\n",
    "- **isnull()** -> Return true iff the column is null \n",
    "- **rand()** -> Generate a random column with independent and identically distributed (i.i.d.) samples uniformly distributed in [0.0, 1.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame-Na Functions\n",
    "- **dropna()** -> Returns a new DataFrame omitting rows with any, all, or a specified number of null values, considering an optional subset of columns\n",
    "- **fill()** -> Replace null values with the specified value for an optional subset of columns\n",
    "- **replace()** -> Returns a new DataFrame replacing a value with another value, considering an optional subset of columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join Functions\n",
    "- **unionByName()** -> Resolves columns by name\n",
    "- **join()** -> Joins two DataFrames based on a given expression\n",
    "\n",
    "Example of joins:  \n",
    "Inner Join Based on one column: df1.join(df2, 'col_1')  \n",
    "Inner Join Based on two columns: df1.join(df2, ['col_1', 'col_2'])  \n",
    "Full Outer Join based on one column: df1.join(df2, 'col_1', 'outer')  \n",
    "Left Outer Join based on one column: df1.join(df2, df1['col_1'] == df2['col_2'] 'left_outer')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Defined Functions (UDF)\n",
    "- Can't be optimized by Catalyst Optimizer\n",
    "- Function must be serialized and sent to executors\n",
    "- Overhead from Python interpreter on executors running Python UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function:\n",
    "\n",
    "def first_letter_func(col_1):  \n",
    "    return col_1[0]  \n",
    "\n",
    "#Register the UDF (this will serialize the function):\n",
    "flfUDF = udf(first_letter_func)\n",
    "\n",
    "#Using the UDF:\n",
    "display(df.select(flfUDF(col(\"col_12\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Register in SQL:\n",
    "flfUDF = spark.udf.register(\"sql_flf_udf\", first_letter_func)\n",
    "\n",
    "#By doing this, you will still be able to use the Python version\n",
    "\n",
    "#Using the UDF in SQL:\n",
    "SELECT sql_flf_udf(col_1) AS firstletter\n",
    "FROM table_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Register as Decorator (@udf)\n",
    "@udf(\"string\")\n",
    "def first_letter_func(col_1: str) -> str:  \n",
    "    return col_1[0]  \n",
    "#Pandas UDF\n",
    "@pandas_udf(\"string\")\n",
    "def vectorizedUDF(col_1: pd.Series) -> pd.Series:\n",
    "    return col_1.str[0]\n",
    "\n",
    "#OR\n",
    "\n",
    "def vectorizedUDF(col_1: pd.Series) -> pd.Series:\n",
    "    return col_1.str[0]\n",
    "vectorizedUDF = pandas_udf(vectorizedUDF, \"string\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45328cea26b25948b97b0f37f1a12ccfd1fd6868ffd79945aa9cfb5de8c131d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
